---
title: "Vetiver Research"
date: August 2022
format: 
    html: 
        toc: true
---

# Motivation

MLOps is a new space. Although we had come with varying levels of knowledge and experience, it had been a while since any of us had dedicated time to explicitly work on research and step back once again to understand the ecosystem around what MLOps is and how it is perceived. 

# What did we want to learn?

To avoid the trap of researching with no end goal, we wanted to be driven by certain research questions. In general,

1. Is there anything in our vocabulary we should change?
2. Where are our shortcomings? 
3. Are these things that are out of scope, or that need more investment?

# Method

With the questions in mind, we needed to find works to help us answer them. As a team, we put together a list of works we wanted to spend time reading. We dedicated the month of August to reading, with a synthesis meeting the first week of September to talk about what we learned and our answers to the questions posed.

We decided to all read one longer work together, __Designing Machine Learning Systems__ by Chip Nguyen. This is a full-length book on MLOps, looking over the entire model lifecycle. We also all brought in links to other, shorter articles and split them up between ourselves.

![](images/readinglist.png)
We also had a number of content pieces that were simply looking at people's explanations of what MLOps is, to better understand expetations of what general public' psychological contract with the word MLOps is. 


# Discussion

One paper that seemed relevant was the "The ML Test Score: A Rubric for ML Production Readiness and Technical Debt Reduction" which laid out categories and a point system for users who were creating MLOps systems to score themselves on how well they had put everything together. 

We also found projects labeled as MLOps frameworks covered a broad scope. Projects generally fell into a few different categories:
  - Pipelines
  - Experiment tracking and versioning
  - Model serving
  - Dashboarding
  - All-in-one

## Current standing

> _1. Is there anything in our vocabulary we should change?_

After our research, we feel confident in our choice to focus on the practices of versioning, deploying, and monitoring. We also want to continue to stress the importance of modularity and composability with MLOps tools. 

## Scope

> _2. Where are our shortcomings?_
> _3. Are these things that are out of scope, or that need more investment?_

We feel DAG support is out of scope of vetiver. We feel model registries may become important, but they are not currently in our short-term plan. Currently, to make a model registry for R objects, you could use a mix of `pins` and `htmlwidgets` and this may become easy to implement in Python with Quarto.

## Intended future investments

> _2. Where are our shortcomings?_
> _3. Are these things that are out of scope, or that need more investment?_

We hope to invest more time in monitoring, but need to balance the fact that a lot of the CI/CD in monitoring is infrastructure dependent.

The language of MLOps is still developing. Many terms such as "batch processing" have multiple conflicting meanings. We proposed to create a glossary of how we use terms in vetiver's documentation. (or put this in current standing?)

We also feel it is important to inspect the incorporation of vetiver components with other MLOps projects, such as MLFlow or Metaflow.

We would like to use the "The ML Test Score: A Rubric for ML Production Readiness and Technical Debt Reduction" as a framework for to best understand where vetiver "wins points" to help explain to usefulness of the project in terms of best practices.

# Final remarks

After reading, we feel confident in vetiver's scope of versioning, deploying, and monitoring models. We feel vetiver's distinctive strengths lie in its ability to quickly prototype REST APIs, and then scale these prototypes safely. We think there is more work to be done in monitoring, especially with respect to needs for continuous monitoring.

# Our reading list

Books:

-   [*Designing Machine Learning Systems*](https://www.oreilly.com/library/view/designing-machine-learning/9781098107956/) by Chip Huyen

Articles:

-   ["Machine Learning Operations (MLOps): Overview, Definition, and Architecture"](https://arxiv.org/abs/2205.02302) by Kreuzberger et al
-   ["From Concept Drift to Model Degradation: An Overview on Performance-Aware Drift Detectors"](https://arxiv.org/abs/2203.11070) by Bayram et al
-   ["Towards Observability for Production Machine Learning Pipelines"](https://arxiv.org/pdf/2108.13557.pdf) by Shankar et al
-   ["The ML Test Score: A Rubric for ML Production Readiness and Technical Debt Reduction"](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/aad9f93b86b7addfea4c419b9100c6cdd26cacea.pdf) by Breck et al

Web content:

-   [*How ML Breaks: A Decade of Outages for One Large ML Pipeline*](https://www.youtube.com/watch?v=hBMHohkRgAA) by Papasian and Underwood
-   [*MLOps Principles*](https://ml-ops.org/content/mlops-principles) by INNOQ
-   [*Google's Practitioners Guide to MLOps*](https://services.google.com/fh/files/misc/practitioners_guide_to_mlops_whitepaper.pdf) by Salama et al
-   [*Gently Down the Stream*](https://www.gentlydownthe.stream/) by Mitch Seymour
